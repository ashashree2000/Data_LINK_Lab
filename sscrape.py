# -*- coding: utf-8 -*-
"""sscrape.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nxB9kJOoq2DW5lxcferHyTrRTEznqEqv
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import Select
import time
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

url = "https://himkosh.nic.in/eHPOLTIS/PublicReports/wfrmBudgetAllocationbyFD.aspx"

# Create a webdriver instance (assuming you have ChromeDriver installed)
driver = webdriver.Chrome()
driver.get(url)
# Send an HTTP GET request to the website
response = requests.get(url)
content = response.content


# Wait for the page to fully load (adjust the timeout as needed)
WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'txtFromDate')))
# Select From Date
from_date = driver.find_element(By.ID, "txtFromDate")
from_date.clear()
from_date.send_keys("01/04/2018")

# Select To Date
to_date = driver.find_element(By.ID, "txtQueryDate")
to_date.clear()
to_date.send_keys("31/03/2022")

# Select Report Data as per Demand and HOA Wise Summary
# report_data = Select(driver.find_element(By.ID, "ctl00_ContentPlaceHolder1_ddlReportData"))
# report_data.select_by_visible_text("Demand and HOA Wise Summary")

# Select Unit as Rupees
unit = driver.find_element(By.ID, "MainContent_rbtUnit_0")
unit.click()
# unit.select_by_visible_text("Rupees")
# Find and click the submit button
submit_button = driver.find_element(By.ID, "btnGetdata")
submit_button.click()

# Wait for the report to load (you may need to adjust the time.sleep duration)
time.sleep(5)
# Parse the HTML content using Beautiful Soup
soup = BeautifulSoup(content, 'html.parser')

# Find the table containing the data (you may need to inspect the website's HTML structure)
table = soup.find('table')

# Extract data from the table into a list of lists
data = []
for row in table.find_all('td'):
    cols = row.find_all('td')
    cols = [col.text.strip() for col in cols]
    data.append(cols)

# Convert the data into a Pandas DataFrame
df = pd.DataFrame(data)
print(data)
driver.get(url)